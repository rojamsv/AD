# -*- coding: utf-8 -*-
"""Advanced time series forecasting

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gDlJFAS2jeviQ9fo-AfzKF8inRkcCkDQ
"""

import os
import math
import random
import time
from copy import deepcopy


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader


from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error


# SARIMAX baseline
import statsmodels.api as sm
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except Exception:
    PROPHET_AVAILABLE = False


# Reproducibility
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)


DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
###############################################################################
# 1) Programmatic dataset generation
###############################################################################


def generate_multivariate_series(n_steps=2500, n_features=3, freq=24, trend_strength=0.001, noise_scale=0.2):
    """Generate correlated multivariate non-stationary time series with trend, seasonality and cross-feature correlation.
    freq: seasonality period (e.g., 24 for daily if hourly data)
    """
    t = np.arange(n_steps)


    # base seasonality (shared)
    base_season = 2.0 * np.sin(2 * np.pi * t / freq) + 0.5 * np.cos(2 * np.pi * t / (freq*0.5))


    # slowly changing trend
    trend = trend_strength * (t ** 1.2)


    data = np.zeros((n_steps, n_features))


    # diversity in feature shapes + coupling
    for i in range(n_features):
        phase = i * (np.pi/6)
        amp = 1.0 + 0.5 * i
        # feature-specific seasonality plus shared base
        feat_season = amp * np.sin(2 * np.pi * t / freq + phase) + 0.3 * base_season
        # autoregressive-like dependence on previous features
        coupling = 0.0
        if i > 0:
            coupling = 0.2 * np.roll(feat_season, 1) # naive coupling
        noise = noise_scale * np.random.randn(n_steps)
        data[:, i] = feat_season + trend + coupling + noise


    # Clip/extreme scaling for realism
    data = data.astype(np.float32)
    return pd.DataFrame(data, columns=[f'feat_{i}' for i in range(n_features)])

###############################################################################
# 2) Dataset, sequences and scaler
###############################################################################


class TimeSeriesDataset(Dataset):
    def __init__(self, df, input_len=96, forecast_horizon=24, scaler=None):
        # df: pandas DataFrame (n_steps, n_features)
        self.input_len = input_len
        self.forecast_horizon = forecast_horizon
        self.n = len(df)
        self.features = df.columns.tolist()
        self.X = df.values.astype(np.float32)
        self.scaler = scaler or StandardScaler()
        # Fit scaler on training portion externally (user must pass fitted scaler for train/test split)


    def __len__(self):
        return max(0, self.n - self.input_len - self.forecast_horizon + 1)


    def __getitem__(self, idx):
        x = self.X[idx: idx + self.input_len]
        y = self.X[idx + self.input_len: idx + self.input_len + self.forecast_horizon]
        return x, y


def make_datasets(df, train_ratio=0.8, input_len=96, forecast_horizon=24):
    n = len(df)
    train_end = int(n * train_ratio)
    train_df = df.iloc[:train_end]
    test_df = df.iloc[train_end - input_len - forecast_horizon + 1:]
    # Fit scaler on training
    scaler = StandardScaler()
    scaler.fit(train_df.values)
    train_scaled = pd.DataFrame(scaler.transform(train_df.values), columns=df.columns, index=train_df.index)
    test_scaled = pd.DataFrame(scaler.transform(test_df.values), columns=df.columns, index=test_df.index)


    train_ds = TimeSeriesDataset(train_scaled, input_len=input_len, forecast_horizon=forecast_horizon, scaler=scaler)
    test_ds = TimeSeriesDataset(test_scaled, input_len=input_len, forecast_horizon=forecast_horizon, scaler=scaler)
    return train_ds, test_ds, scaler, train_end

    ###############################################################################
# 3) Positional encoding for time series
###############################################################################


class TimePositionalEncoding(nn.Module):
    """Positional encoding adapted for continuous time steps and variable embedding dims.
    We'll combine standard sinusoidal encoding with a learned linear projection so model can tune time signals.
    Input: (batch, seq_len, d_model) or we accept d_model to produce (seq_len, d_model)
    """
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0) # (1, max_len, d_model)
        self.register_buffer('pe', pe)


    def forward(self, x):
        # x: (batch, seq_len, d_model)
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len, :].to(x.device)
        return self.dropout(x)
        ###############################################################################
# 4) Sequence-to-sequence Transformer Model (encoder-decoder)
###############################################################################


class Seq2SeqTransformer(nn.Module):
    def __init__(self, feature_size, d_model=128, nhead=8, num_encoder_layers=3, num_decoder_layers=3,
                 dim_feedforward=512, dropout=0.1, forecast_horizon=24):
        super().__init__()
        self.feature_size = feature_size
        self.d_model = d_model
        self.input_proj = nn.Linear(feature_size, d_model)
        self.output_proj = nn.Linear(d_model, feature_size)
        self.pos_enc = TimePositionalEncoding(d_model)


        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,
                                          num_encoder_layers=num_encoder_layers,
                                          num_decoder_layers=num_decoder_layers,
                                          dim_feedforward=dim_feedforward,
                                          dropout=dropout,
                                          batch_first=True)
        self.forecast_horizon = forecast_horizon


    def forward(self, src, tgt=None):
        # src: (batch, src_len, feature_size)
        batch_size = src.size(0)
        src_emb = self.input_proj(src) * math.sqrt(self.d_model)
        src_emb = self.pos_enc(src_emb)


        if tgt is None:
            # autoregressive decoding: start with zeros and generate sequence in parallel using teacher forcing off
            # For simplicity we will prepare a zero tensor of length forecast_horizon
            tgt_seq = torch.zeros((batch_size, self.forecast_horizon, self.feature_size), device=src.device)
            tgt_emb = self.input_proj(tgt_seq) * math.sqrt(self.d_model)
            tgt_emb = self.pos_enc(tgt_emb)
            memory = self.transformer.encoder(src_emb)
            out = self.transformer.decoder(tgt_emb, memory)
            out = self.output_proj(out)
            return out
        else:
            # training with teacher forcing: tgt provided
            tgt_emb = self.input_proj(tgt) * math.sqrt(self.d_model)
            tgt_emb = self.pos_enc(tgt_emb)
            memory = self.transformer.encoder(src_emb)
            out = self.transformer.decoder(tgt_emb, memory)
            out = self.output_proj(out)
            return out

      ###############################################################################
# 5) Training loop, metrics and helpers
###############################################################################


def mape(y_true, y_pred):
    denom = np.maximum(np.abs(y_true), 1e-8)
    return np.mean(np.abs((y_true - y_pred) / denom)) * 100.0



def evaluate_model(model, dataloader, scaler, device=DEVICE):
    model.eval()
    preds = []
    trues = []
    with torch.no_grad():
        for x, y in dataloader:
            x = x.to(device)
            y = y.to(device)
            out = model(x) # (batch, horizon, features)
            preds.append(out.cpu().numpy())
            trues.append(y.cpu().numpy())
    preds = np.concatenate(preds, axis=0)
    trues = np.concatenate(trues, axis=0)
    # inverse scale
    bs, h, f = preds.shape
    preds_flat = preds.reshape(-1, f)
    trues_flat = trues.reshape(-1, f)
    preds_inv = scaler.inverse_transform(preds_flat).reshape(bs, h, f)
    trues_inv = scaler.inverse_transform(trues_flat).reshape(bs, h, f)


    rmse = np.sqrt(mean_squared_error(trues_inv.flatten(), preds_inv.flatten()))
    mae = mean_absolute_error(trues_inv.flatten(), preds_inv.flatten())
    mape_v = mape(trues_inv.flatten(), preds_inv.flatten())
    return {'rmse': rmse, 'mae': mae, 'mape': mape_v}, preds_inv, trues_inv



def train_loop(model, train_loader, val_loader, scaler, epochs=50, lr=1e-4,
             weight_decay=1e-5, device=DEVICE, patience=5, model_path='best_model.pt'):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)
    criterion = nn.MSELoss()


    best_val = float('inf')
    best_state = None
    no_improve = 0
    history = {'train_loss': [], 'val_rmse': []}


    for epoch in range(1, epochs + 1):
        model.train()
        running = 0.0
        cnt = 0
        t0 = time.time()
        for x, y in train_loader:
            x = x.to(device)
            y = y.to(device)
            optimizer.zero_grad()
            # Teacher forcing: feed ground-truth target as decoder input
            out = model(x, y)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
            running += loss.item() * x.size(0)
            cnt += x.size(0)
        train_loss = running / cnt
        val_metrics, _, _ = evaluate_model(model, val_loader, scaler, device=device)
        val_rmse = val_metrics['rmse']
        scheduler.step(val_rmse)
        history['train_loss'].append(train_loss)
        history['val_rmse'].append(val_rmse)


        if val_rmse < best_val:
            best_val = val_rmse
            best_state = deepcopy(model.state_dict())
            torch.save(best_state, model_path)
            no_improve = 0
        else:
            no_improve += 1


        print(f"Epoch {epoch}/{epochs} â€” train_loss: {train_loss:.6f} val_rmse: {val_rmse:.4f} time: {time.time()-t0:.1f}s")
        if no_improve >= patience:
            print('Early stopping')
            break


    if best_state is not None:
        model.load_state_dict(best_state)
    return model, history
###############################################################################
# 6) Baseline: SARIMAX and optionally Prophet
###############################################################################


def sarimax_forecast(train_series, test_series, order=(1,1,1), seasonal_order=(0,1,1,24)):
    # train_series: pandas Series (single feature). We'll create one-step-ahead rolling forecasts to match multi-step horizon.
    model = sm.tsa.statespace.SARIMAX(train_series, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)
    res = model.fit(disp=False)
    preds = res.forecast(steps=len(test_series))
    return preds


def prophet_forecast(train_df, periods):
    # train_df must have columns ds (datetime) and y
    if not PROPHET_AVAILABLE:
        raise RuntimeError('Prophet not available. pip install prophet')
    m = Prophet()
    m.fit(train_df)
    future = m.make_future_dataframe(periods=periods, freq='H')
    fc = m.predict(future)
    return fc['yhat'].iloc[-periods:]
    ###############################################################################
# 7) Sensitivity analysis utility
###############################################################################


def sensitivity_analysis(df, input_len=96, horizon=24, param_grid=None, train_ratio=0.8):
    if param_grid is None:
        param_grid = [
            {'d_model':64, 'nhead':4},
            {'d_model':128, 'nhead':8},
            {'d_model':256, 'nhead':8},
        ]
    results = []
    for params in param_grid:
        print('\nRunning sensitivity config:', params)
        train_ds, test_ds, scaler, train_end = make_datasets(df, train_ratio=train_ratio, input_len=input_len, forecast_horizon=horizon)
        train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
        val_loader = DataLoader(test_ds, batch_size=64, shuffle=False)
        model = Seq2SeqTransformer(feature_size=df.shape[1], d_model=params['d_model'], nhead=params['nhead'], forecast_horizon=horizon)
        model, history = train_loop(model, train_loader, val_loader, scaler, epochs=30, lr=1e-4, patience=4, model_path=f"best_{params['d_model']}_{params['nhead']}.pt")
        metrics, preds, trues = evaluate_model(model, val_loader, scaler)
        print('Result:', metrics)
        results.append({'params': params, 'metrics': metrics, 'history': history})
    return results
###############################################################################
# 8) Visualization & saving helpers
###############################################################################


def plot_history(history, save_path=None):
    plt.figure(figsize=(8,4))
    plt.plot(history['train_loss'], label='train_loss')
    plt.plot(history['val_rmse'], label='val_rmse')
    plt.legend()
    plt.title('Training history')
    if save_path:
        plt.savefig(save_path)
    plt.show()


def plot_forecast(trues, preds, feature_idx=0, n_plot=500):
    # trues and preds are (batch, horizon, features), we'll flatten in time order for plotting the test portion
    trues_flat = trues.reshape(-1, trues.shape[-1])[:, feature_idx]
    preds_flat = preds.reshape(-1, preds.shape[-1])[:, feature_idx]
    t = np.arange(len(trues_flat))
    plt.figure(figsize=(12,4))
    plt.plot(t[:n_plot], trues_flat[:n_plot], label='true')
    plt.plot(t[:n_plot], preds_flat[:n_plot], label='pred')
    plt.legend()
    plt.title(f'Forecast vs True (feature {feature_idx})')
    plt.show()

    ###############################################################################
# 9) Main runnable experiment
###############################################################################


def run_full_experiment():
    # Generate data
    df = generate_multivariate_series(n_steps=2500, n_features=3, freq=24)
    print('Generated df shape:', df.shape)


    # Hyperparameters
    input_len = 96
    horizon = 24
    train_ds, test_ds, scaler, train_end = make_datasets(df, train_ratio=0.8, input_len=input_len, forecast_horizon=horizon)
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    val_loader = DataLoader(test_ds, batch_size=64, shuffle=False)


    model = Seq2SeqTransformer(feature_size=df.shape[1], d_model=128, nhead=8, num_encoder_layers=3, num_decoder_layers=3, forecast_horizon=horizon)
    model, history = train_loop(model, train_loader, val_loader, scaler, epochs=50, lr=1e-4, patience=6, model_path='best_transformer.pt')


    metrics, preds, trues = evaluate_model(model, val_loader, scaler)
    print('Transformer test metrics:', metrics)
    plot_history(history)
    plot_forecast(trues, preds, feature_idx=0, n_plot=400)


    # Baseline: SARIMAX per feature (we'll do simple one-step forecasting rolled to match horizon for demonstration)
    # For fairness we'd need to implement multivariate SARIMAX or VAR; but we'll do univariate per-feature baseline and average metrics
    sarimax_metrics = {}
    for i, feat in enumerate(df.columns):
        # prepare series split
        series = df[feat]
        train_series = series.iloc[:train_end]
        test_series = series.iloc[train_end:]
        try:
            preds = sarimax_forecast(train_series, test_series)
            rmse = np.sqrt(mean_squared_error(test_series, preds))
            mae = mean_absolute_error(test_series, preds)
            mape_v = mape(test_series.values, preds.values)
            sarimax_metrics[feat] = {'rmse': rmse, 'mae': mae, 'mape': mape_v}
        except Exception as e:
            print('SARIMAX failed for', feat, e)
            sarimax_metrics[feat] = None
    print('SARIMAX per-feature metrics:', sarimax_metrics)


    # Sensitivity analysis
    param_grid = [{'d_model':64,'nhead':4}, {'d_model':128,'nhead':8}, {'d_model':256,'nhead':8}]
    sens_res = sensitivity_analysis(df, input_len=input_len, horizon=horizon, param_grid=param_grid)


    return {
        'transformer_metrics': metrics,
        'sarimax_metrics': sarimax_metrics,
        'sensitivity': sens_res
    }


if __name__ == '__main__':
    results = run_full_experiment()
    print('\nExperiment finished. Results summary:')
    print(results)